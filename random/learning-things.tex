\documentclass[]{article}
%
\usepackage[utf8]{inputenc} % below are various important packages
\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage[english]{babel}
\usepackage{textcomp} 
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage{amssymb}	
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{scrlayer-scrpage}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{framed}
\usepackage{hyperref} 
\usepackage{pgf,tikz,pgfplots} % possibility to insert geogebra graphs
\usepackage{mathrsfs}
\pgfplotsset{compat=1.15}\usetikzlibrary{arrows} % part of geogebra package
\usepackage{qrcode} % insert qr codes
\usepackage{multicol}
\usepackage{yfonts}
\usepackage{multirow}
\usepackage{stmaryrd}
\usepackage{xurl}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{float}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsthm}

\renewcommand{\rmdefault}{phv}

\usepackage{titling}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\setlength{\droptitle}{-10em}

% Add to length for wider margins
\addtolength{\textwidth}{5cm} % right to margin
\addtolength{\hoffset}{-2.5cm} % left to margin
\addtolength{\voffset}{-1.5cm} % to top
\addtolength{\textheight}{3.5cm} % to bottom

\linespread{1.2} % increase line spacing

\begin{document}
\title{Book of things}
\author{Luisa Cicolini}
\maketitle 

\section{Books}

\subsection{the method of coalgebra - j. rutten }

\paragraph{algebras}

\begin{definition}
    A Functor $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{D}$, where $\mathcal{C}$ and $\mathcal{D}$ are categories, 
    assigns (1) to any object $A\in \mathcal{C}$ an object $\mathcal{F}(A)\in \mathcal{D}$, (2) to any arrow $f:A\rightarrow B\in \mathcal{C}$ an 
    arrow $\mathcal{F}(f) : \mathcal{F}(A) \rightarrow \mathcal{F}(B) \in \mathcal{D}$, such that (3) $\mathcal{F}$ preserves composition and identies. 
\end{definition}

\begin{definition}
    Let $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. An $\mathcal{F}-$algebra is a pair $(A,\alpha)$ consisting of an obhect $A$ and an arrow 
    $\alpha:\mathcal{F}(A)\rightarrow A$. $\mathcal{F}$ is the type, $A$ is the carrier, $\alpha$ is the structure map of the algebra.
\end{definition}

\paragraph{example} $(\mathbb{N}, [zero, succ])$ is an $N$-algebra, defined via functor $N:Set\rightarrow Set$ for everty set $X$ by $N(X)=1+X$

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An homomorphism of $F$-algebras $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $f\circ \alpha = \beta\circ F(f)$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $F(A)\xrightarrow{\alpha}A$
        \item $F(B)\xrightarrow{\beta}B$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}

\paragraph{coalgebras}
\begin{definition}
    Let $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. An $\mathcal{F}-$coalgebra is a pair $(A,\alpha)$ consisting of an obhect $A$ and an arrow 
    $\alpha:A\rightarrow \mathcal{F}(A)$. $\mathcal{F}$ is the type, $A$ is the carrier, $\alpha$ is the structure map of the coalgebra.
\end{definition}

Coalgebras are like algebras, but the structure map is reversed.

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An homomorphism of $F$-algebras $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $\beta\circ f = F(f)\circ \alpha$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $A\xrightarrow{\alpha}F(A)$
        \item $B\xrightarrow{\beta}F(B)$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}

Coalgebras are the dual form of algebra and are derived via the categorical principle of duality.

\paragraph{inductive and coinductive definitions}

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An initial $F$-algebra is an $F$-algebra that is an \textit{initial object} in the category of all $F$-algebras and $F$ $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $\beta\circ f = F(f)\circ \alpha$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $A\xrightarrow{\alpha}F(A)$
        \item $B\xrightarrow{\beta}F(B)$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}


\subsection{modern automata theory - z. \'esik}

\paragraph{finite automata} 
\begin{definition}
    A semiring is a set $A$ equipped with two binary operations $+$ and $\cdot$ and two constant elements $0,1$ such that:
    \begin{enumerate}
        \item $\langle A, +, 0\rangle$ is a commutative monoid (= set of elements with an associative, commutative binary operation and an identity element)
        \item $\langle A, \cdot, 1\rangle$ is a monoid (= set of elements with an associative binary operation and an identity element)
        \item the following distribution laws hold for all elements: $a \cdot (b+c) = a \cdot b + a \cdot c$, $(a + b)\cdot c = a\cdot c + b\cdot c$
        \item $0\cdot a = a\cdot 0 = 0$ for every $a$
    \end{enumerate}
\end{definition}

\begin{definition}
    A starsemiring is a semiring equipped with an additional unary operation $*$. Example of starsemirings: $\langle \mathbb{B}, +, \cdot, *, 0, 1 \rangle$ with $0*=1*=1$
\end{definition}

\begin{definition}
    A Conway semiring is a starsemiring that satisfies the sum-star-equation:
    \begin{equation}
        (a+b)^* = (a^*b)^*a^*
    \end{equation}
     and the product-star-equation:
    \begin{equation}
        (ab)^* = 1 + a(ba)^*b
    \end{equation}
     Example: semiring $\langle 2^{\Sigma^*}, \cup, \cdot, *, \emptyset, \{ \epsilon \} \rangle$ of formal languages over $\Sigma$ with $L^*=\cup_{n\geq 0}L^n$ for all $L \subseteq \Sigma^*$
\end{definition}

A way to highlight the connection between graphs and automata:

\begin{definition}
    Consider a Conway semiring $A$ and its subset $A'$. 
    A finite automaton $A'$-automaton $\textfrak{U}=(n,M,S,P), n \geq 1$ is given by:
    \begin{enumerate}
        \item a transition matrix $M\in (A' \cup \{0,1\})^{n\times n}$
        \item an initial state vector $S\in (A' \cup \{0,1\})^{1\times n}$
        \item a final state vector $P\in (A' \cup \{0,1\})^{n\times 1}$
    \end{enumerate}
    The behavior $||\textfrak{U}||$ of $\textfrak{U}$ is defined by 
    \begin{equation}
        ||\textfrak{U}|| = \Sigma_{1 \leq i_1, i_2 \leq n} S_{i_1} (M^*)_{i_1,i_2} P_{i_2} = S M^* P
    \end{equation}
\end{definition}

\paragraph{context-free grammars and algebraic systems}

\section{Papers}

\subsection{finite presentations of infinite structures: automata and interpretations - a. blumensath et al.}

\paragraph{definitions}

Some domanis of infinite buf finitely presentable structures: 
\begin{itemize}
    \item recursive structure: countable structures whose functions and relations are computable, 
        their domain is typically too large 
    \item constraint database: modern database model admitting infinite relations presented by quantifier-free formulae
        over a fized background structure. A constraint database consists of a context structure $\textfrak{U}$ and a set $\{\phi_1, ..., \phi_m\}$ of quantifier-free formulae defining the database relations
    \item metafinite strucutres: two-sorted structures consisting of a finite structure $\textfrak{U}$, a background structure $\textfrak{R}$ (usually infinite but fixed) and a class of weight functions from finite to infinite part. For example: finite graphs whose edges are weighted by real numbers. 
    \item automatic structures: their functions and relations are represented by finite automata. A relational strucutren $\textfrak{U}=(A, R_1, ..., R_m)$ is automatic if there exists a regular language $L_\delta \subseteq \Sigma^*$ naming the elements in $\textfrak{U}$
        and a function $\nu : L_\delta \rightarrow A$ mapping every $w\in L_\delta$ to the element of $\textfrak{U}$ that it represents. 
        $\nu$ must be surjective but necessarily injective (everything has a name, can have more than one). 
        Finite automata must be able to recognize (1) whether wro words in $L_\delta$ name the same elements and (2) for each $R_i \in \textfrak{U}$ whether a given tuple of words in $L_\delta$ names a tuple in $R_i$.
        Using automata over infinite words we obtain $\omega-$automatic structures, which may have uncountable cardinality. 
        Overall, automatic structures admit effective and automatic evaluation on all first-order queries. 
        \begin{theorem}
            The model-checking problem for $FO(\exists^\omega)$, first order logic extended by quantifier "there are infinitely many" is decidable on the domain of $\omega$-automatic structures. 
        \end{theorem}
    \item tree-automatic structures: defined by automata on finite or infinite trees, natural generalization of automatic structures 
    \item tree-interpretable structures: structures that are interpretable on the infinite binary tree $\mathcal{T}^2=(\{0,1\}^*, \sigma_0, \sigma_1)$ via one-dimensional monadic second-order interpretation. 
        they form a proper subclass of automatic structures that generalizes various notions of infinite graphs. Examples: context free graphs (= configuration graphs of PDA), HR- and VR-equational graphs, prefix-recognizable graphs.
        \begin{theorem}
            For any graph $G=(V,(E_a)_{a\in A})$ the followign are equivalent: 
            (1) G is tree-interpretable,
            (2) G is prefix-recognizable,
            (3) G is VR-equational,
            (4) G is the restriction to a regular set of the configuration graph of a PDA with $\epsilon$-transitions.
        \end{theorem}
    \item tree-constructible structures: Caucal hierarchy
    \item ground tree rewriting graphs 
\end{itemize}
\begin{definition}
    A relational structure $\textfrak{U}$ is automatic if there exists a regular language $L_\delta \subseteq \Sigma^*$ and a surjective function $\nu : L_\delta \rightarrow A$ such that the relation 
    \begin{equation}
        L_\epsilon := \{(w, w')\in :L_\delta \times L_\delta\;|\;\nu w = \nu w'\}\subseteq \Sigma^* \times \Sigma^*
    \end{equation}
    and, for all predicates $R\subseteq A^r$ of $\textfrak{U}$, the relations 
    \begin{equation}
        L_R := \{\overline{w}\in (L_\delta)^r\;|\;(\nu w_1,...,\nu w_r)\in R\}\subseteq (\Sigma^*)^r
    \end{equation}
    are regular. An arbitrary structure is automatic iif its relational variant is. 
\end{definition}
All finite structures are automatic, and all automatic structures are $\omega$-automatic.
\begin{definition}
    Let $L$ be a logic and $\textfrak{U}=(A, R_0,..., R_n)$ and $\textfrak{B}$ relational structures. A (k-dimensional) L-interpretation of $\textfrak{U}$ in $\textfrak{B}$ is a sequence
    \begin{equation}
        \mathcal{I} := \langle \delta(\overline{x}), \epsilon(\overline{x}, \overline{y}), \phi_{R_0}(\overline{x}_1,...,\overline{x}_r),...., \phi_{R_n}(\overline{x}_1,...,\overline{x}_s)\rangle
    \end{equation}
    of L-formulae of the vocabulary of $\textfrak{B}$ (where each tuple $\overline{x}, \overline{y}, \overline{x_i}$ consists of k variables) such that
    \begin{equation}
        \textfrak{U} \cong \mathcal{I}(\textfrak{B}) := (\delta^\textfrak{B},(\phi_{R_0})^\textfrak{B},...,(\phi_{R_n})^\textfrak{B})/\epsilon^\textfrak{B}
    \end{equation}
\end{definition}
\begin{theorem}
    If $\mathcal{I} : \textfrak{U}\leq_{FO} \textfrak{B}$, then 
    \begin{equation}
        \textfrak{U} \models \phi (\mathcal{I}{\overline{b}})\;\; iif \;\; \textfrak{B}\models \phi^\mathcal{I} (\overline{b}) \;\; for\; all\; \phi\in FO\;\; and \overline{b}\subseteq \delta^\textfrak{B} 
    \end{equation}
\end{theorem}
This theorem states that for any logic L a notion of interpretation is suitable if a similar statement holds and if the logic is closed under $\phi \mapsto \phi^\mathcal{I}$
The classes of autimatic, $\omega$-automatic structures are closed under 
(1) extensions by definable relations, (2) factorizations by definable congruences, (3) substructures with definable universe and (4) finite powers. 

\paragraph{model checking and query evaluation}
\begin{itemize}
    \item model checking: given a structure $\textfrak{U}$, a formula $\phi(\overline{x})$ and a tuple of parameters $\overline{a}$ in $\textfrak{U}$, decide whethere $\textfrak{U}\models \phi(\overline{a})$
    \item query evaluation: given a presentation of a structure $\textfrak{U}$ and a formula $\phi(\overline{x})$, compute a presentation of $(\textfrak{U},\phi^\textfrak{U})$, i.e., given automata for the relations of $\textfrak{U}$ 
    construct an automaton that recognizes $\phi^\textfrak{U}$.
\end{itemize}
All first-order queries on ($\omega-$)automatic structures are computable since: 
\begin{theorem}
    If $\textfrak{U}\leq \textfrak{B}$ and $\textfrak{B}$ is ($\omega$-)automatic, then so is $\textfrak{U}$.
\end{theorem}
Every ($\omega$-)automatic structure has an injective presentation.
Reachability is undecidable for automatic structures. 
It is undedicable whether two automatic structures are isomorphic.

\subsection{abstract interpretation from B\"uchi Automata - m. hoffman et al.}
From a given BA, build an abstract lattice with the following properties: 
\begin{itemize}
    \item there is a Galois connection between it and the infinite lattice of languages of finite and infinite words over a given alphabet
    \item the abstraction is faithful wrt. acceptance
    \item least fixpoints and $\omega-$iterations can be computed on the level of the abstract lattice
\end{itemize}
one can develop an abstract interpretation to check whether finite and infinite traces of a recursive program are accepted by a policy automaton. 
this approach is more flexible for integration with data types, objects, higher-order functions (easier reasoning?)

\section{b\"uchi, lindenbaum, tarski: a program analysis appetizer - v. d'silva et al.}
There are various approaches to prove the correctness of a program:
\begin{itemize}
    \item satisfiability-based: bounded executions and errors of $P$ are encoded as a formulae $Exec(P)$ and $Err$, respectively. 
        If no bounded execution violates the assertion, then: $\vdash Exec(P)\implies \neg Err $. Solvers prove this by showing $Exec(P)\land Err$ unsatisfiable.
    \item model checking: check whether program $P$ is a model of formula $\neg Err$: $P\models \neg Err$
    \item automata-theoretic: the executions of program $P$ are words accepted by automaton $\mathcal{A}_P$, erroneous executions are words accepted by $\mathcal{A}_{Err}$. Program 
        $P$ contains no assertion violation if $\mathcal{L}(\mathcal{A}_P\times {A}_{Err})=\emptyset$
    \item lattice-theoretic: originated from programming language semantics and compiler construction, relies on abstract interpretation to interpret program $P$ and assertion $Err$ in a lattice A of approximation. 
        The program is error-free if the lattice element denoted by $P$ is separate from the lattice element denoted by the error: $\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$
\end{itemize}
One can move in between these representations:
\begin{itemize}
    \item automata ($\mathcal{L}(\mathcal{A}_{P}\times \mathcal{A}_{Err})\subseteq \emptyset$) $\xrightarrow{b\"uchi's theorem}$ logic ($\vdash Exec(P)\implies \neg Err $)
    \item automata ($\mathcal{L}(\mathcal{A}_{P}\times \mathcal{A}_{Err})\subseteq \emptyset$) $\xrightarrow{\mathcal{L}}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)
    \item logic ($\vdash Exec(P)\implies \neg Err $) $\xrightarrow{mod}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)
    \item logic ($\vdash Exec(P)\implies \neg Err $) $\xrightarrow{Lindenbaum-Tarski}$ abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$)
    \item concrete lattice ($\mathcal{P}(Exec), \subseteq$) $\xrightarrow{abs}$ abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$)
    \item abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$) $\xrightarrow{conc}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)  
\end{itemize}
Loop invariants are fixed points of functions on lattices. 

\subsection{a sat-based procedure for verifying finite state machines in ACL2 - w. hunt et al.}
\begin{itemize}
    \item sulfa properties converted into sat (cnf): 
    \item We present an algorithm for converting ACL2 conjectures into conjunctive normal form (CNF),
        which we then output and check with an external satisfiability solver. 
        The procedure is directly available as an ACL2 proof request. 
        When the SAT tool is successful, a theorem  is added to the ACL2 system database as a lemma for use
        in future proof attempts. 
\end{itemize}

\subsection{cell morphing: from array programs to array-free horn clauses - d. monniaux et al.}

From our programs with arrays, we generate nonlinear Horn clauses over scalar variables only,
    in a common format with clear and unambiguous logical semantics, for which there exist several
    solvers. We thus avoid the use of solvers operating over arrays, which are still very immature.

\subsection{inductive approach to spacer - t. tsukada et al.}

\begin{itemize}
    \item as linear CHC: an approach called property-directed reachability proposed as a solver of finite model-checking 
    that corresponds to linear CHCs over finite data domain, also applied to non linear CHCs (GPDR)
    \item Spacer is based on several new ideas, but the key to refutational completeness is a technique 
    called model-based projection. It is used to divide the set of local candidates
    of counterexamples into a finite number of classes, and the finiteness of the classes allows an
    exhaustive search for candidates of global counterexamples in a finite number of steps.
    \item the refutational completeness of spacer seems to be a matter of discussion? there exists a refutationally
    complete variant
\end{itemize}

\subsection{formal verification at higher levels of abstraction - d. kroening et al., 2007}

\begin{itemize}
    \item Algorithms that operate at the bit-level are unable to
    exploit the structure provided by the higher abstraction levels,
    and thus, are less scalable $\rightarrow$ high level models: (1) word-level verification with predicate abstraction and smt solvers, (2) term-level modeling and their combination for scalable verification 
    \item Abstraction techniques reduce
    the state space by mapping the set of states of the actual,
    concrete system to an abstract, and smaller, set of states in a
    way that preserves the relevant behaviors of the system $\rightarrow$ might be nice to have hierarchy in this?
    \item Capacity is the main challenge for formal verification
    tools. Given a high-level model, word-level reasoning can
    increase the capacity of formal verification tools significantly
    when compared to a net-list level tool. We discuss decision
    procedures (SMT solvers) for bit-vector arithmetic, and give
    an overview of predicate abstraction, a word-level assertion
    checking technique.
\end{itemize}

\subsection{high-level abstractions and modular debugging for FPGA design validation - y. iskander et al., 2014}
The developed approach provides two means of directly validating synthesized hardware designs.
The first allows the original high-level model written in C or C++ to be directly coupled to the synthesized
hardware, abstracting away the traditional gate-level view of designs. A high-level programmatic interface
allows the synthesized design to be validated directly by the software reference model. The second approach
provides an alternative view to FPGAs within the scope of a traditional software debugger. This debug
framework leverages partially reconfigurable regions to accelerate the modification of dynamic, software-like
breakpoints for low-level analysis and provides a automatable, scriptable, command-line interface directly
to a running design on an FPGA.

\subsection{fsm anomaly detection using formal analysis - f. farahmandi et al., 2017}

The proposed method tries to find inconsistencies between the specification and FSM implementation through manipulation of respective polynomials.
Security properties (such as a safe transition to a protected state) are derived using specification polynomials and verified against implementation polynomials. In a case of a failure, the vulnerability is reported. While existing methods can verify legal transitions, our approach tries to solve the important and non-trivial problem of detecting illegal accesses to the design states (e.g., protected states). 

\subsection{spot: an extensible model checking library using transition-based generalize b\"uchi automata - a. duret-lutx et al., 2004}

b\"uchi automata $\rightarrow$ tableaux before encoding 

\subsection{eliminating excessive dynamism of dataflow circuits using model cheecking - xu et al., 20223}

In this work, we present a verification framework based on model checking to systematically reduce the hardware complexity of dataflow circuits.

\section{thoughts and ideas}

\begin{itemize}
    \item a compositional approach to hardware verification exploiting high level abstractions (dialects)
        for progressive and verified lowering, using fsms as a proxy to represent dialects' semantics, 
        based on the assumption that the automata-theoretic approach is better than the logic one (in this specific case? in general? based on what?).
    \item so for example a problem with logic representation is that it scales quite poorly with the number of variables involved
    \item a decision procedure such as the one we have already is easily extensible to multiple variables? 
    \item the key difference between the two approaches is that for the logic formulae we need to use smt solvers, which are typically very good under 
        very constrained circumstances (e.g. fsm x chc), instead when we have an automaton we can "just" explore the automaton and use some 
        heuristics to optimize its exploration and prune the state space. which solvers do too so idk if there's a real benefit in there. 
        but certainly representing things as automata is easier than massaging an smt formula. 
    \item in theory we can use automatic structures on domains that are broader than bool, although i suppose their complexity scales quite poorly. 
    \item Things we want to say and do over the three years: 
        (1) formalize high level abstractions as FSMs (papers on how to use FSMs to represent dataflow? requires representing actual handshake)
        (2) extend fsm dialect to represent buechi automata 
        (3) reason about designs in terms of omega-languages, ltl and ctl
        (4) make verification of designs modular and prpogressive as the design becomes more complex.
    \item there are three fundamentally equivalent ways to analyze a program and verify it: automata-theoretic approach, logic approach, abstract interpretation. 
        we can move between the three, but beign able to exploit their characteristics and differences to perform and optimize different analyses could make our life esier. 
        automata are good for (1) temporal logic stuff and (2) concurrent designs etc, so how am i planning to use this in a compiler thingy? 
        in hardware modeling ltl stuff is useful! but here we're talking about compilers? 
    \item can fsms help in modeling the gray area between dataflow and rtl?
    \item fsms are good for LTL and temporal logic. we already have smt for model checking. for temporal properties (which, crucially, are very important for hw) buechi automata are our best shot. 
        integrating BA in circt would give us another very powerful tool for verification, complementing the work that is currently being done with smt but with a focus on temporal (modal?) logics. 
        having this in a framework that is by design modular and reusability oriented is a great combo because we can exploit all the algorithms that enable to easily manipulate automata (a well scoped and known problem)
        and this would make our life easier. furthermore, if we encode the so-obtained automata in an itp such as lean, we might even get the additional benefit that we can reason about it, write theorems, 
        prove the lowering correct. 
    \item dialects semantics is just a sequence of operations really. can we encode that in some sort of automaton like the ones we have for bitvectors? 
    \item itps can be a game changer in (1) the progressive verificat on of hw designs and (2) the verification of lowerings and optimization sin compilers
    \item buechi automata as an easier interface to reason about ltl properties on the design?
    \item does it make sense to use buechi automata to reason about ?
\end{itemize}

\begin{figure}
    \centering 
    \includegraphics[width=0.8\columnwidth]{proposal-idea.pdf}
\end{figure}

\paragraph{ITPs to Democratize Hardware Verification}

% polished abstract 

Hardware design is an inherently complex task, requiring a through understanding of numerous components and tools. 
Verification plays a crucial role in this process, due to the 

Hardware design is well known to be a complex task, in which verification plays a fundamental role due to the enormous risks and costs involved in printing mistakes. 

Domain Specific Languages (DSLs) are powerful tools that can significantly ease the design process, allowing to reason about a design at different abstraction levels.

The CIRCT framework comprises various dialects that represents different hardware abstractions (e.g. FSMs and dataflow circuits) and the lowerings from one to another, 
making the compilation of high-level representation of a design into a low-level output (e.g. SystemVerilog) progressive.

And this approach has proven successful because it fosters reusability and progressive optimization of the designs (it is currently employed in real-world hardware design)



Integrating verification in one such framework is a complex task that requires taking into account the semantics of the different dialects, as well as the correctness of all 
the lowerings and optimization steps involved in the compilation of a single design. 

Promising verification efforts in CIRCT currently rely on 
(1) the SMT dialect that can encode the semantics of different dialects in SMT formuale, 
in an attempt to extract useful informati~on from higher-level abstractions and enable a progressive verification of the design
(2) an ad-hoc unrolling bounded model checker that checks designs at RTL level. 
(3) the AIG dialect, a novel addition to the circt ecosytstem, that exploits AIG as most hardware model checkers do, 
(4) lowering from RTL to BTOR2 representation

while lowering them to SMT formulae proved a good approach, but suffers from scalability issues (SMT solvers are very good in a very restricted field of problem e.g. arithmetic constraints)
complementing this approach with more high level reasoning about the semantics (see the three equivalent ways we have to prove things) allows fully exploiting the information we can extract at higher abstraction levels
and simultaneously verify the lowerings and the designs 

MOreover, there is always some kind of interface or abstraction that we need to use as a middle ground to encode the semantics of the problem we are trying to solve: 

And in fact, none of these approaches allows to flexibly reason about and massage the semantics of dialects, to reason about the equivalence between models 
and in particular about the semantics in cases that represent typical grey areas, where different representations converge (e.g. latency-insensitive vs. cycle-based visions).

Moreover, confining the verification of a design to the bottom of the design process means (1) discarding all the structural and semantics information present at higher abstraction 
levels of abstractions, which could instead guide the verification procedure and (2) preventing designers from safely taking advantage of different thought paradigms (e.g. dataflow) that call for 
unique sets of optimizations and characteristics. 

Currently, taking advantage of the semantics of the higher level dialects to guide the lower-level verification steps is currently complex and uniquely relies 
on the smt dialect. 

However, formalizign the semantics of CIRCT is generally a complex task because of how different the dialects are. MOreover the reason why this currently has not been done is because reasoning about the semantics of such a tool that encombasses many different representation working in wildly different 
ways is complex (there is no much documentation) 


overall we want to : 
(1) let high-level information about the circuit influence and guide the verification progressive as the lowering happens 
(2) let the reasoning about high-level semantics influence fill the gaps that currently exists since the semantics is not well formalized.
semantics that are not well formalized are a proxy for mistakes
(3) exploit the formalized semantics to reason about the lowerings' correctness
it is important to define semantics carefully and to take full advantage of it, which is why thinking very intensely about the semantics of circt dialects, taking extra care in formalizing their semantics such that it is correct and expressive enough 
to reason about the subtle border between different abstractions (e.g. latency insensitive vs. circuit oriented). semsntics are powerful if you knwo how to use them

and we want these things to unlock 
(1) the possibility of verifying properties at higher level and 
(2) ensure that the prop holds throughout the lowering as long as the semantics are correctly formalized 

starting from the circt abstractions that define two significantlyt different ways to think about circuits dataflow and fsm -> put them together and formalize the gray area laying in between different abstractions
and concretely a framework where one can easily add new things and what not 

as a stretch goal, this shall open the doors to integrating further verification techniquest (e.g. automata-theoretic approaches)




however we have a powerful tool on our side: lean4 makes it easier because with tools such as lean-mlir we can formalize the semantics of a dialects 
and easily test it, test the correctness of the semantics given the examples in the framework and verify the lowerings themselves too. 





% chatGPT's envisioning of the thing 




Hardware design is inherently complex, with verification playing a critical role in mitigating the high risks and costs associated with errors. 
Domain-Specific Languages (DSLs) streamline this process by enabling designers to reason at various levels of abstraction. 
CIRCT, a framework for hardware compilation, leverages multiple dialects (e.g., FSMs, dataflow circuits) and systematic lowering techniques to transform high-level 
representations into lower-level outputs like SystemVerilog, promoting reusability and progressive optimization.

Integrating verification into CIRCT presents unique challenges due to the need to formalize and validate the semantics of diverse dialects, as well as the 
correctness of transformations and optimizations applied during compilation. Current efforts include:

    The SMT Dialect, which encodes semantics into SMT formulas for progressive verification.
    An RTL-Level Bounded Model Checker, tailored for specific scenarios.
    The AIG Dialect, which adopts And-Inverter Graphs for model checking.
    Lowerings to BTOR2, a standard representation for hardware verification.

While these approaches are effective in isolation, they are limited by scalability issues and lack a unifying semantic framework to reason about equivalences and grey areas (e.g., latency-insensitive vs. cycle-based paradigms). 
Furthermore, confining verification to low-level representations discards valuable structural and semantic insights present at higher abstraction levels, 
hindering designers from fully leveraging different paradigms like dataflow and FSM.

To address these gaps, we propose a systematic formalization of CIRCT semantics that enables:

    High-level semantic guidance for progressive verification.
    Robust reasoning about dialects and their transformations, bridging gaps in current methodologies.
    Validation of lowerings through well-defined and expressive semantics, particularly across subtle abstraction boundaries.

This approach aims to unlock the ability to verify properties at high levels of abstraction and ensure their preservation throughout the lowering process. 
Lean4 and tools like Lean-MLIR can facilitate this effort, enabling precise formalization and automated testing of dialect semantics and transformations. 
Long-term, this foundational work could support integration of advanced verification techniques, such as automata-theoretic approaches, and foster the creation of a modular framework for evolving hardware verification.







% what's the solution 

% the solution is that i spend the next four years of my life 

% what's the outcome 




% Hardware desigin is complex, frameworks such as CIRCT try to make the process smoother, fostering reusability 
% among components and making the compilation progressive, using DSLs (dialects) that simplify reasoning
% and optimizing a certain abstraction. 

% Now combining this idea with verification (which is fundamental in hardware) is really not easy, 
% dialects represent very different abstaction levels relying on different paradigms (e.g. latency insensitive vs. cycle based)
% there are currently efforts towards verification in circt. 
% however, due to the complexity and differences in the semantics at different layer, extra attention must be put into 
% thinking about the formalization of the semantics, if we really want to build a verification flows that
% can exytract information from higher levels 


% It is common knowledge that hardware design is a complex field standing on a wide variety of different tools. 
% Hardware verification is a fundamental aspect of hardware design, since we only want to actually print the circuits that work as expected.
% On the one hand, improving the efficiency of hardware design by making the verification progressive and parallel to the lowering can significantly improve and speed up the overall design procedure.
% On the other hand, the verification of the compilation toolchain itself is a valuable complement to the verification of the design, which would significantly improve the trustworthiness of these tools and 
% ease the verification of designs themselves. 
% Moreover, if a lowering is verified and proven to be correct, a property we are checking at a certain level will hold at a lower level, too. 

% For this reason, numerous works have attempted to simplify and accelerate the verification of designs and propose end-to-end verified compilers.
% However, no solution currently takes advantage of both strategies. // i think this is a lie

% We want to be able to reason about the semantics of the lowerings and of the design at the same time and within the same context. 
% ITPs, and in particular Lean4, are extremely flexible tool that allows encoding the semantics at different stages of the compilation and reasoning about the equivalence of designs 
% at different abstraction levels, as well as the correctness of the actual lowering taking place 

% --- 

% % set conrext
% A single flaw in hardware that is taped out from verilog leads to billions of losses.
% Thus, correctness is paramount for hardware development, 
% and hardware verification forms a cornerstone of this industry.
% % what's the problem?
% Such extremely robust verification tools are built by reducing the entire circuit to humongous SAT formulae,
% where impressive advances in SAT solving since the 90s allows solvers to solve problems will millions of instances,
% thereby making SAT solving for industrial chip design a reality.
% % the problem:
% However, a key disadvantage of this approach is that all higher-level structure of the circuit is lost,
% and indeed, SAT solvers often need to reconstruct even basic logic gates such as if-then-else and XOR from the SAT formulae themselves.
% % why is this hard to solve?
% On the other hand, encoding higher-level concepts into the solver rapidly increases the surface area for bugs,
% both in the specification of these high level encodings,
% as well as in challenge to the solver posed by non-uniformity of the input language.
% % What's the solution?
% We believe that breaking this barrier, and enabling solvers to exploit high-level constructs in circuits fearlessly
% will yield large, potentially order of magnitude speedups.
% % 
% To break this barrier, we propose to employ a \emph{formally specified} semantics for these higher level constructs,
% and we also propose to develop \emph{verified} solvers for these high-level constructs.
% Concretely, we propose to use the Lean proof assistant,
% where the \texttt{lean=mlir} project already provides a formal specification of higher-level hardware semantics
% (co-developed by me), and has recently gained a verified SAT solver (\texttt{bv\_decide}),
% upon whose foundation we shall build verified SMT solvers for these higher level constructs.
% In particular, we are interested in building both verified reductions to various theories
% (e.g. CHC, LTL/CTL(*)) as well as building verified solvers for these theories in Lean. 
% % what's the outcome?
% We belive that this will break the abstraction barrier in hardware verification,
% and will enable large-scale,
% trusted solvers that effectively exploit the high level structure of the circuit,
% leaving no stone unturned and no subcircuit wasted!

% \paragraph{the importance of verification in compilers}
% \begin{itemize}
%     \item crucial components: we need to (1) verify their lowerings and (2) verify their output 
%     \item especially in hardware "the end of moore's law etc", formal verification is a crucial step and often a bottleneck. would be nice to make it progressive and verify
%     \item automata-based verification makes no sense to verify the compiler per se, since there's no ltl property that we want to check within the lowering 
%     \item denote the semantics of dialect as automata? 
% \end{itemize}

% \paragraph{verification techniques and their use in compilers}
% \begin{itemize}
%     \item program analysis techniques (e.g. abstract interpretation )
% \end{itemize}

% \paragraph{why lean}
% \begin{itemize}
%     \item easy compositional approach, easy to extend, customize, relatively easy to write theorems to prove equivalence using lean-mlir 
% \end{itemize}

% \paragraph{research proposal and challenges}

% \begin{itemize}
%     \item convert a design into a buechi automaton and verify a property progressively as the design is lowered and compiled 
%     \item compiler verification: extract dialects semantics using buechi automata 
%     \item automata-based approach to the verification of compiler lowerings and designs at higher level of abstractions 
%     \item can we encode the semantics of a dialect in an omega automaton?
% \end{itemize}

% \paragraph{timeline}
% \begin{itemize}
%     \item add most circt dialect to lean-mlir, verify lowerings
%     \item write (in lean) a tool that can extract a buechi automaton from a generic circt design in a certain dialect
%     \item define buechi automata structure and conversion 
% \end{itemize}
% \paragraph{conclusion} 

\end{document}