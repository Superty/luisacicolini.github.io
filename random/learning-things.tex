\documentclass[]{article}
%
\usepackage[utf8]{inputenc} % below are various important packages
\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage[english]{babel}
\usepackage{textcomp} 
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage{amssymb}	
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{scrlayer-scrpage}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{framed}
\usepackage{hyperref} 
\usepackage{pgf,tikz,pgfplots} % possibility to insert geogebra graphs
\usepackage{mathrsfs}
\pgfplotsset{compat=1.15}\usetikzlibrary{arrows} % part of geogebra package
\usepackage{qrcode} % insert qr codes
\usepackage{multicol}
\usepackage{yfonts}
\usepackage{multirow}
\usepackage{stmaryrd}
\usepackage{xurl}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{float}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsthm}

\renewcommand{\rmdefault}{phv}

\usepackage{titling}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\setlength{\droptitle}{-10em}

% Add to length for wider margins
\addtolength{\textwidth}{5cm} % right to margin
\addtolength{\hoffset}{-2.5cm} % left to margin
\addtolength{\voffset}{-1.5cm} % to top
\addtolength{\textheight}{3.5cm} % to bottom

\linespread{1.2} % increase line spacing

\begin{document}
\title{Book of things}
\author{Luisa Cicolini}
\maketitle 

\section{Books}

\subsection{the method of coalgebra - j. rutten }

\paragraph{algebras}

\begin{definition}
    A Functor $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{D}$, where $\mathcal{C}$ and $\mathcal{D}$ are categories, 
    assigns (1) to any object $A\in \mathcal{C}$ an object $\mathcal{F}(A)\in \mathcal{D}$, (2) to any arrow $f:A\rightarrow B\in \mathcal{C}$ an 
    arrow $\mathcal{F}(f) : \mathcal{F}(A) \rightarrow \mathcal{F}(B) \in \mathcal{D}$, such that (3) $\mathcal{F}$ preserves composition and identies. 
\end{definition}

\begin{definition}
    Let $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. An $\mathcal{F}-$algebra is a pair $(A,\alpha)$ consisting of an obhect $A$ and an arrow 
    $\alpha:\mathcal{F}(A)\rightarrow A$. $\mathcal{F}$ is the type, $A$ is the carrier, $\alpha$ is the structure map of the algebra.
\end{definition}

\paragraph{example} $(\mathbb{N}, [zero, succ])$ is an $N$-algebra, defined via functor $N:Set\rightarrow Set$ for everty set $X$ by $N(X)=1+X$

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An homomorphism of $F$-algebras $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $f\circ \alpha = \beta\circ F(f)$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $F(A)\xrightarrow{\alpha}A$
        \item $F(B)\xrightarrow{\beta}B$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}

\paragraph{coalgebras}
\begin{definition}
    Let $\mathcal{F}: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. An $\mathcal{F}-$coalgebra is a pair $(A,\alpha)$ consisting of an obhect $A$ and an arrow 
    $\alpha:A\rightarrow \mathcal{F}(A)$. $\mathcal{F}$ is the type, $A$ is the carrier, $\alpha$ is the structure map of the coalgebra.
\end{definition}

Coalgebras are like algebras, but the structure map is reversed.

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An homomorphism of $F$-algebras $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $\beta\circ f = F(f)\circ \alpha$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $A\xrightarrow{\alpha}F(A)$
        \item $B\xrightarrow{\beta}F(B)$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}

Coalgebras are the dual form of algebra and are derived via the categorical principle of duality.

\paragraph{inductive and coinductive definitions}

\begin{definition}
    Let $F: \mathcal{C} \rightarrow \mathcal{C}$ be a functor. 
    An initial $F$-algebra is an $F$-algebra that is an \textit{initial object} in the category of all $F$-algebras and $F$ $(A,\alpha)$, $(B,\beta)$ is an arrow $f:A\rightarrow B$ such that $\beta\circ f = F(f)\circ \alpha$:
    \begin{itemize}
        \item $F(A)\xrightarrow{F(f)}F(B)$
        \item $A\xrightarrow{\alpha}F(A)$
        \item $B\xrightarrow{\beta}F(B)$
        \item $A\xrightarrow{f}B$
    \end{itemize}
    For this definition to make sense $F$ must be a functor and act not only on objects, but also on arrows.
\end{definition}


\subsection{modern automata theory - z. \'esik}

\paragraph{finite automata} 
\begin{definition}
    A semiring is a set $A$ equipped with two binary operations $+$ and $\cdot$ and two constant elements $0,1$ such that:
    \begin{enumerate}
        \item $\langle A, +, 0\rangle$ is a commutative monoid (= set of elements with an associative, commutative binary operation and an identity element)
        \item $\langle A, \cdot, 1\rangle$ is a monoid (= set of elements with an associative binary operation and an identity element)
        \item the following distribution laws hold for all elements: $a \cdot (b+c) = a \cdot b + a \cdot c$, $(a + b)\cdot c = a\cdot c + b\cdot c$
        \item $0\cdot a = a\cdot 0 = 0$ for every $a$
    \end{enumerate}
\end{definition}

\begin{definition}
    A starsemiring is a semiring equipped with an additional unary operation $*$. Example of starsemirings: $\langle \mathbb{B}, +, \cdot, *, 0, 1 \rangle$ with $0*=1*=1$
\end{definition}

\begin{definition}
    A Conway semiring is a starsemiring that satisfies the sum-star-equation:
    \begin{equation}
        (a+b)^* = (a^*b)^*a^*
    \end{equation}
     and the product-star-equation:
    \begin{equation}
        (ab)^* = 1 + a(ba)^*b
    \end{equation}
     Example: semiring $\langle 2^{\Sigma^*}, \cup, \cdot, *, \emptyset, \{ \epsilon \} \rangle$ of formal languages over $\Sigma$ with $L^*=\cup_{n\geq 0}L^n$ for all $L \subseteq \Sigma^*$
\end{definition}

A way to highlight the connection between graphs and automata:

\begin{definition}
    Consider a Conway semiring $A$ and its subset $A'$. 
    A finite automaton $A'$-automaton $\textfrak{U}=(n,M,S,P), n \geq 1$ is given by:
    \begin{enumerate}
        \item a transition matrix $M\in (A' \cup \{0,1\})^{n\times n}$
        \item an initial state vector $S\in (A' \cup \{0,1\})^{1\times n}$
        \item a final state vector $P\in (A' \cup \{0,1\})^{n\times 1}$
    \end{enumerate}
    The behavior $||\textfrak{U}||$ of $\textfrak{U}$ is defined by 
    \begin{equation}
        ||\textfrak{U}|| = \Sigma_{1 \leq i_1, i_2 \leq n} S_{i_1} (M^*)_{i_1,i_2} P_{i_2} = S M^* P
    \end{equation}
\end{definition}

\paragraph{context-free grammars and algebraic systems}

\section{Papers}

\subsection{finite presentations of infinite structures: automata and interpretations - a. blumensath et al.}

\paragraph{definitions}

Some domanis of infinite buf finitely presentable structures: 
\begin{itemize}
    \item recursive structure: countable structures whose functions and relations are computable, 
        their domain is typically too large 
    \item constraint database: modern database model admitting infinite relations presented by quantifier-free formulae
        over a fized background structure. A constraint database consists of a context structure $\textfrak{U}$ and a set $\{\phi_1, ..., \phi_m\}$ of quantifier-free formulae defining the database relations
    \item metafinite strucutres: two-sorted structures consisting of a finite structure $\textfrak{U}$, a background structure $\textfrak{R}$ (usually infinite but fixed) and a class of weight functions from finite to infinite part. For example: finite graphs whose edges are weighted by real numbers. 
    \item automatic structures: their functions and relations are represented by finite automata. A relational strucutren $\textfrak{U}=(A, R_1, ..., R_m)$ is automatic if there exists a regular language $L_\delta \subseteq \Sigma^*$ naming the elements in $\textfrak{U}$
        and a function $\nu : L_\delta \rightarrow A$ mapping every $w\in L_\delta$ to the element of $\textfrak{U}$ that it represents. 
        $\nu$ must be surjective but necessarily injective (everything has a name, can have more than one). 
        Finite automata must be able to recognize (1) whether wro words in $L_\delta$ name the same elements and (2) for each $R_i \in \textfrak{U}$ whether a given tuple of words in $L_\delta$ names a tuple in $R_i$.
        Using automata over infinite words we obtain $\omega-$automatic structures, which may have uncountable cardinality. 
        Overall, automatic structures admit effective and automatic evaluation on all first-order queries. 
        \begin{theorem}
            The model-checking problem for $FO(\exists^\omega)$, first order logic extended by quantifier "there are infinitely many" is decidable on the domain of $\omega$-automatic structures. 
        \end{theorem}
    \item tree-automatic structures: defined by automata on finite or infinite trees, natural generalization of automatic structures 
    \item tree-interpretable structures: structures that are interpretable on the infinite binary tree $\mathcal{T}^2=(\{0,1\}^*, \sigma_0, \sigma_1)$ via one-dimensional monadic second-order interpretation. 
        they form a proper subclass of automatic structures that generalizes various notions of infinite graphs. Examples: context free graphs (= configuration graphs of PDA), HR- and VR-equational graphs, prefix-recognizable graphs.
        \begin{theorem}
            For any graph $G=(V,(E_a)_{a\in A})$ the followign are equivalent: 
            (1) G is tree-interpretable,
            (2) G is prefix-recognizable,
            (3) G is VR-equational,
            (4) G is the restriction to a regular set of the configuration graph of a PDA with $\epsilon$-transitions.
        \end{theorem}
    \item tree-constructible structures: Caucal hierarchy
    \item ground tree rewriting graphs 
\end{itemize}
\begin{definition}
    A relational structure $\textfrak{U}$ is automatic if there exists a regular language $L_\delta \subseteq \Sigma^*$ and a surjective function $\nu : L_\delta \rightarrow A$ such that the relation 
    \begin{equation}
        L_\epsilon := \{(w, w')\in :L_\delta \times L_\delta\;|\;\nu w = \nu w'\}\subseteq \Sigma^* \times \Sigma^*
    \end{equation}
    and, for all predicates $R\subseteq A^r$ of $\textfrak{U}$, the relations 
    \begin{equation}
        L_R := \{\overline{w}\in (L_\delta)^r\;|\;(\nu w_1,...,\nu w_r)\in R\}\subseteq (\Sigma^*)^r
    \end{equation}
    are regular. An arbitrary structure is automatic iif its relational variant is. 
\end{definition}
All finite structures are automatic, and all automatic structures are $\omega$-automatic.
\begin{definition}
    Let $L$ be a logic and $\textfrak{U}=(A, R_0,..., R_n)$ and $\textfrak{B}$ relational structures. A (k-dimensional) L-interpretation of $\textfrak{U}$ in $\textfrak{B}$ is a sequence
    \begin{equation}
        \mathcal{I} := \langle \delta(\overline{x}), \epsilon(\overline{x}, \overline{y}), \phi_{R_0}(\overline{x}_1,...,\overline{x}_r),...., \phi_{R_n}(\overline{x}_1,...,\overline{x}_s)\rangle
    \end{equation}
    of L-formulae of the vocabulary of $\textfrak{B}$ (where each tuple $\overline{x}, \overline{y}, \overline{x_i}$ consists of k variables) such that
    \begin{equation}
        \textfrak{U} \cong \mathcal{I}(\textfrak{B}) := (\delta^\textfrak{B},(\phi_{R_0})^\textfrak{B},...,(\phi_{R_n})^\textfrak{B})/\epsilon^\textfrak{B}
    \end{equation}
\end{definition}
\begin{theorem}
    If $\mathcal{I} : \textfrak{U}\leq_{FO} \textfrak{B}$, then 
    \begin{equation}
        \textfrak{U} \models \phi (\mathcal{I}{\overline{b}})\;\; iif \;\; \textfrak{B}\models \phi^\mathcal{I} (\overline{b}) \;\; for\; all\; \phi\in FO\;\; and \overline{b}\subseteq \delta^\textfrak{B} 
    \end{equation}
\end{theorem}
This theorem states that for any logic L a notion of interpretation is suitable if a similar statement holds and if the logic is closed under $\phi \mapsto \phi^\mathcal{I}$
The classes of autimatic, $\omega$-automatic structures are closed under 
(1) extensions by definable relations, (2) factorizations by definable congruences, (3) substructures with definable universe and (4) finite powers. 

\paragraph{model checking and query evaluation}
\begin{itemize}
    \item model checking: given a structure $\textfrak{U}$, a formula $\phi(\overline{x})$ and a tuple of parameters $\overline{a}$ in $\textfrak{U}$, decide whethere $\textfrak{U}\models \phi(\overline{a})$
    \item query evaluation: given a presentation of a structure $\textfrak{U}$ and a formula $\phi(\overline{x})$, compute a presentation of $(\textfrak{U},\phi^\textfrak{U})$, i.e., given automata for the relations of $\textfrak{U}$ 
    construct an automaton that recognizes $\phi^\textfrak{U}$.
\end{itemize}
All first-order queries on ($\omega-$)automatic structures are computable since: 
\begin{theorem}
    If $\textfrak{U}\leq \textfrak{B}$ and $\textfrak{B}$ is ($\omega$-)automatic, then so is $\textfrak{U}$.
\end{theorem}
Every ($\omega$-)automatic structure has an injective presentation.
Reachability is undecidable for automatic structures. 
It is undedicable whether two automatic structures are isomorphic.

\subsection{abstract interpretation from B\"uchi Automata - m. hoffman et al.}
From a given BA, build an abstract lattice with the following properties: 
\begin{itemize}
    \item there is a Galois connection between it and the infinite lattice of languages of finite and infinite words over a given alphabet
    \item the abstraction is faithful wrt. acceptance
    \item least fixpoints and $\omega-$iterations can be computed on the level of the abstract lattice
\end{itemize}
one can develop an abstract interpretation to check whether finite and infinite traces of a recursive program are accepted by a policy automaton. 
this approach is more flexible for integration with data types, objects, higher-order functions (easier reasoning?)

\section{b\"uchi, lindenbaum, tarski: a program analysis appetizer - v. d'silva et al.}
There are various approaches to prove the correctness of a program:
\begin{itemize}
    \item satisfiability-based: bounded executions and errors of $P$ are encoded as a formulae $Exec(P)$ and $Err$, respectively. 
        If no bounded execution violates the assertion, then: $\vdash Exec(P)\implies \neg Err $. Solvers prove this by showing $Exec(P)\land Err$ unsatisfiable.
    \item model checking: check whether program $P$ is a model of formula $\neg Err$: $P\models \neg Err$
    \item automata-theoretic: the executions of program $P$ are words accepted by automaton $\mathcal{A}_P$, erroneous executions are words accepted by $\mathcal{A}_{Err}$. Program 
        $P$ contains no assertion violation if $\mathcal{L}(\mathcal{A}_P\times {A}_{Err})=\emptyset$
    \item lattice-theoretic: originated from programming language semantics and compiler construction, relies on abstract interpretation to interpret program $P$ and assertion $Err$ in a lattice A of approximation. 
        The program is error-free if the lattice element denoted by $P$ is separate from the lattice element denoted by the error: $\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$
\end{itemize}
One can move in between these representations:
\begin{itemize}
    \item automata ($\mathcal{L}(\mathcal{A}_{P}\times \mathcal{A}_{Err})\subseteq \emptyset$) $\xrightarrow{b\"uchi's theorem}$ logic ($\vdash Exec(P)\implies \neg Err $)
    \item automata ($\mathcal{L}(\mathcal{A}_{P}\times \mathcal{A}_{Err})\subseteq \emptyset$) $\xrightarrow{\mathcal{L}}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)
    \item logic ($\vdash Exec(P)\implies \neg Err $) $\xrightarrow{mod}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)
    \item logic ($\vdash Exec(P)\implies \neg Err $) $\xrightarrow{Lindenbaum-Tarski}$ abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$)
    \item concrete lattice ($\mathcal{P}(Exec), \subseteq$) $\xrightarrow{abs}$ abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$)
    \item abstract lattice ($\llbracket P \rrbracket_A \sqcap \llbracket Err \rrbracket_A \sqsubseteq \perp$) $\xrightarrow{conc}$ concrete lattice ($\mathcal{P}(Exec), \subseteq$)  
\end{itemize}
Loop invariants are fixed points of functions on lattices. 

\subsection{a sat-based procedure for verifying finite state machines in ACL2 - w. hunt et al.}
\begin{itemize}
    \item sulfa properties converted into sat (cnf): 
    \item We present an algorithm for converting ACL2 conjectures into conjunctive normal form (CNF),
        which we then output and check with an external satisfiability solver. 
        The procedure is directly available as an ACL2 proof request. 
        When the SAT tool is successful, a theorem  is added to the ACL2 system database as a lemma for use
        in future proof attempts. 
\end{itemize}

\subsection{cell morphing: from array programs to array-free horn clauses - d. monniaux et al.}

From our programs with arrays, we generate nonlinear Horn clauses over scalar variables only,
    in a common format with clear and unambiguous logical semantics, for which there exist several
    solvers. We thus avoid the use of solvers operating over arrays, which are still very immature.

\subsection{inductive approach to spacer - t. tsukada et al.}

\begin{itemize}
    \item as linear CHC: an approach called property-directed reachability proposed as a solver of finite model-checking 
    that corresponds to linear CHCs over finite data domain, also applied to non linear CHCs (GPDR)
    \item Spacer is based on several new ideas, but the key to refutational completeness is a technique 
    called model-based projection. It is used to divide the set of local candidates
    of counterexamples into a finite number of classes, and the finiteness of the classes allows an
    exhaustive search for candidates of global counterexamples in a finite number of steps.
    \item the refutational completeness of spacer seems to be a matter of discussion? there exists a refutationally
    complete variant
\end{itemize}

\subsection{formal verification at higher levels of abstraction - d. kroening et al., 2007}

\begin{itemize}
    \item Algorithms that operate at the bit-level are unable to
    exploit the structure provided by the higher abstraction levels,
    and thus, are less scalable $\rightarrow$ high level models: (1) word-level verification with predicate abstraction and smt solvers, (2) term-level modeling and their combination for scalable verification 
    \item Abstraction techniques reduce
    the state space by mapping the set of states of the actual,
    concrete system to an abstract, and smaller, set of states in a
    way that preserves the relevant behaviors of the system $\rightarrow$ might be nice to have hierarchy in this?
    \item Capacity is the main challenge for formal verification
    tools. Given a high-level model, word-level reasoning can
    increase the capacity of formal verification tools significantly
    when compared to a net-list level tool. We discuss decision
    procedures (SMT solvers) for bit-vector arithmetic, and give
    an overview of predicate abstraction, a word-level assertion
    checking technique.
\end{itemize}

\subsection{high-level abstractions and modular debugging for FPGA design validation - y. iskander et al., 2014}
The developed approach provides two means of directly validating synthesized hardware designs.
The first allows the original high-level model written in C or C++ to be directly coupled to the synthesized
hardware, abstracting away the traditional gate-level view of designs. A high-level programmatic interface
allows the synthesized design to be validated directly by the software reference model. The second approach
provides an alternative view to FPGAs within the scope of a traditional software debugger. This debug
framework leverages partially reconfigurable regions to accelerate the modification of dynamic, software-like
breakpoints for low-level analysis and provides a automatable, scriptable, command-line interface directly
to a running design on an FPGA.

\subsection{fsm anomaly detection using formal analysis - f. farahmandi et al., 2017}

The proposed method tries to find inconsistencies between the specification and FSM implementation through manipulation of respective polynomials.
Security properties (such as a safe transition to a protected state) are derived using specification polynomials and verified against implementation polynomials. In a case of a failure, the vulnerability is reported. While existing methods can verify legal transitions, our approach tries to solve the important and non-trivial problem of detecting illegal accesses to the design states (e.g., protected states). 

\subsection{spot: an extensible model checking library using transition-based generalize b\"uchi automata - a. duret-lutx et al., 2004}

b\"uchi automata $\rightarrow$ tableaux before encoding 

\subsection{eliminating excessive dynamism of dataflow circuits using model cheecking - xu et al., 20223}

In this work, we present a verification framework based on model checking to systematically reduce the hardware complexity of dataflow circuits.

\section{thoughts and ideas}

\begin{itemize}
    \item a compositional approach to hardware verification exploiting high level abstractions (dialects)
        for progressive and verified lowering, using fsms as a proxy to represent dialects' semantics, 
        based on the assumption that the automata-theoretic approach is better than the logic one (in this specific case? in general? based on what?).
    \item so for example a problem with logic representation is that it scales quite poorly with the number of variables involved
    \item a decision procedure such as the one we have already is easily extensible to multiple variables? 
    \item the key difference between the two approaches is that for the logic formulae we need to use smt solvers, which are typically very good under 
        very constrained circumstances (e.g. fsm x chc), instead when we have an automaton we can "just" explore the automaton and use some 
        heuristics to optimize its exploration and prune the state space. which solvers do too so idk if there's a real benefit in there. 
        but certainly representing things as automata is easier than massaging an smt formula. 
    \item in theory we can use automatic structures on domains that are broader than bool, although i suppose their complexity scales quite poorly. 
    \item Things we want to say and do over the three years: 
        (1) formalize high level abstractions as FSMs (papers on how to use FSMs to represent dataflow? requires representing actual handshake)
        (2) extend fsm dialect to represent buechi automata 
        (3) reason about designs in terms of omega-languages, ltl and ctl
        (4) make verification of designs modular and prpogressive as the design becomes more complex.
    \item there are three fundamentally equivalent ways to analyze a program and verify it: automata-theoretic approach, logic approach, abstract interpretation. 
        we can move between the three, but beign able to exploit their characteristics and differences to perform and optimize different analyses could make our life esier. 
        automata are good for (1) temporal logic stuff and (2) concurrent designs etc, so how am i planning to use this in a compiler thingy? 
        in hardware modeling ltl stuff is useful! but here we're talking about compilers? 
    \item can fsms help in modeling the gray area between dataflow and rtl?
    \item fsms are good for LTL and temporal logic. we already have smt for model checking. for temporal properties (which, crucially, are very important for hw) buechi automata are our best shot. 
        integrating BA in circt would give us another very powerful tool for verification, complementing the work that is currently being done with smt but with a focus on temporal (modal?) logics. 
        having this in a framework that is by design modular and reusability oriented is a great combo because we can exploit all the algorithms that enable to easily manipulate automata (a well scoped and known problem)
        and this would make our life easier. furthermore, if we encode the so-obtained automata in an itp such as lean, we might even get the additional benefit that we can reason about it, write theorems, 
        prove the lowering correct. 
    \item dialects semantics is just a sequence of operations really. can we encode that in some sort of automaton like the ones we have for bitvectors? 
    \item itps can be a game changer in (1) the progressive verificat on of hw designs and (2) the verification of lowerings and optimization sin compilers
    \item buechi automata as an easier interface to reason about ltl properties on the design?
    \item does it make sense to use buechi automata to reason about ?
\end{itemize}

\begin{figure}
    \centering 
    \includegraphics[width=0.8\columnwidth]{proposal-idea.pdf}
\end{figure}

\paragraph{title: ITPs for hardware verification}

It is common knowledge that hardware design is a complex field standing on a wide variety of different tools. 

Hardware verification is a fundamental aspect of this procedure. 

On the one hand, improving its efficiency by making the verification progressive and parallel to the lowering can significantly improve and speed up the overall design procedure.

On the other hand, the verification of the compilation toolchain itself is a valuable complement to the verification of the design, which would significantly improve the trustworthiness of these tools and 
ease the verification of designs themselves. 

However, reasoning about the semantics of such a complex procedure proved quite complex, due to the variety of behaviors involved in the lowering.

ITPs can provide an effective support in reasoning about such semantics, providing it is encoded correctly, allowing to flexibly verify the equivalence of designs
at different levels of abstractions.

\paragraph{the importance of verification in compilers}
\begin{itemize}
    \item crucial components: we need to (1) verify their lowerings and (2) verify their output 
    \item especially in hardware "the end of moore's law etc", formal verification is a crucial step and often a bottleneck. would be nice to make it progressive and verify
    \item automata-based verification makes no sense to verify the compiler per se, since there's no ltl property that we want to check within the lowering 
    \item denote the semantics of dialect as automata? 
\end{itemize}

\paragraph{verification techniques and their use in compilers}
\begin{itemize}
    \item program analysis techniques (e.g. abstract interpretation )
\end{itemize}

\paragraph{why lean}
\begin{itemize}
    \item easy compositional approach, easy to extend, customize, relatively easy to write theorems to prove equivalence using lean-mlir 
\end{itemize}

\paragraph{research proposal and challenges}

\begin{itemize}
    \item convert a design into a buechi automaton and verify a property progressively as the design is lowered and compiled 
    \item compiler verification: extract dialects semantics using buechi automata 
    \item automata-based approach to the verification of compiler lowerings and designs at higher level of abstractions 
    \item can we encode the semantics of a dialect in an omega automaton?
\end{itemize}

\paragraph{timeline}
\begin{itemize}
    \item add most circt dialect to lean-mlir, verify lowerings
    \item write (in lean) a tool that can extract a buechi automaton from a generic circt design in a certain dialect
    \item define buechi automata structure and conversion 
\end{itemize}
\paragraph{conclusion} 

\end{document}